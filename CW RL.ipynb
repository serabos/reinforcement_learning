{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "20IyxDzgp3tU"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt # Graphical library\n",
    "#from sklearn.metrics import mean_squared_error # Mean-squared error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Fr69C0UBQk"
   },
   "source": [
    "# Coursework 1 :\n",
    "See pdf for instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QsKvVllvvreH"
   },
   "outputs": [],
   "source": [
    "# WARNING: fill in these two functions that will be used by the auto-marking script\n",
    "# [Action required]\n",
    "\n",
    "def get_CID():\n",
    "  return \"02303294\" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)\n",
    "\n",
    "def get_login():\n",
    "  return \"sb722\" # Return your short imperial login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKEz3d9NUbdO"
   },
   "source": [
    "## Helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWnMW3GNpjd7"
   },
   "outputs": [],
   "source": [
    "# This class is used ONLY for graphics\n",
    "# YOU DO NOT NEED to understand it to work on this coursework\n",
    "\n",
    "class GraphicsMaze(object):\n",
    "\n",
    "  def __init__(self, shape, locations, default_reward, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):\n",
    "\n",
    "    self.shape = shape\n",
    "    self.locations = locations\n",
    "    self.absorbing = absorbing\n",
    "\n",
    "    # Walls\n",
    "    self.walls = np.zeros(self.shape)\n",
    "    for ob in obstacle_locs:\n",
    "      self.walls[ob] = 20\n",
    "\n",
    "    # Rewards\n",
    "    self.rewarders = np.ones(self.shape) * default_reward\n",
    "    for i, rew in enumerate(absorbing_locs):\n",
    "      self.rewarders[rew] = 10 if absorbing_rewards[i] > 0 else -10\n",
    "\n",
    "    # Print the map to show it\n",
    "    self.paint_maps()\n",
    "\n",
    "  def paint_maps(self):\n",
    "    \"\"\"\n",
    "    Print the Maze topology (obstacles, absorbing states and rewards)\n",
    "    input: /\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(self.walls + self.rewarders)\n",
    "    plt.show()\n",
    "\n",
    "  def paint_state(self, state):\n",
    "    \"\"\"\n",
    "    Print one state on the Maze topology (obstacles, absorbing states and rewards)\n",
    "    input: /\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    states = np.zeros(self.shape)\n",
    "    states[state] = 30\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(self.walls + self.rewarders + states)\n",
    "    plt.show()\n",
    "\n",
    "  def draw_deterministic_policy(self, Policy):\n",
    "    \"\"\"\n",
    "    Draw a deterministic policy\n",
    "    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze\n",
    "    for state, action in enumerate(Policy):\n",
    "      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
    "        continue\n",
    "      arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
    "      action_arrow = arrows[action] # Take the corresponding action\n",
    "      location = self.locations[state] # Compute its location on graph\n",
    "      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
    "    plt.show()\n",
    "\n",
    "  def draw_policy(self, Policy):\n",
    "    \"\"\"\n",
    "    Draw a policy (draw an arrow in the most probable direction)\n",
    "    input: Policy {np.array} -- policy to draw as probability\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])\n",
    "    self.draw_deterministic_policy(deterministic_policy)\n",
    "\n",
    "  def draw_value(self, Value):\n",
    "    \"\"\"\n",
    "    Draw a policy value\n",
    "    input: Value {np.array} -- policy values to draw\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze\n",
    "    for state, value in enumerate(Value):\n",
    "      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value\n",
    "        continue\n",
    "      location = self.locations[state] # Compute the value location on graph\n",
    "      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph\n",
    "    plt.show()\n",
    "\n",
    "  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):\n",
    "    \"\"\"\n",
    "    Draw a grid representing multiple deterministic policies\n",
    "    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for subplot in range (len(Policies)): # Go through all policies\n",
    "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy\n",
    "      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze\n",
    "      for state, action in enumerate(Policies[subplot]):\n",
    "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
    "          continue\n",
    "        arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
    "        action_arrow = arrows[action] # Take the corresponding action\n",
    "        location = self.locations[state] # Compute its location on graph\n",
    "        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
    "      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument\n",
    "    plt.show()\n",
    "\n",
    "  def draw_policy_grid(self, Policies, title, n_columns, n_lines):\n",
    "    \"\"\"\n",
    "    Draw a grid representing multiple policies (draw an arrow in the most probable direction)\n",
    "    input: Policy {np.array} -- array of policies to draw as probability\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])\n",
    "    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)\n",
    "\n",
    "  def draw_value_grid(self, Values, title, n_columns, n_lines):\n",
    "    \"\"\"\n",
    "    Draw a grid representing multiple policy values\n",
    "    input: Values {np.array of np.array} -- array of policy values to draw\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for subplot in range (len(Values)): # Go through all values\n",
    "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value\n",
    "      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze\n",
    "      for state, value in enumerate(Values[subplot]):\n",
    "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value\n",
    "          continue\n",
    "        location = self.locations[state] # Compute the value location on graph\n",
    "        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph\n",
    "      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbY8DCqoVJlw"
   },
   "source": [
    "## Maze class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXc1OFvZqJfZ"
   },
   "outputs": [],
   "source": [
    "# This class define the Maze environment\n",
    "\n",
    "class Maze(object):\n",
    "\n",
    "  # [Action required]\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Maze initialisation.\n",
    "    input: /\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    \n",
    "    # [Action required]\n",
    "    # Properties set from the CID\n",
    "    self._prob_success = 0.8 # float\n",
    "    self._gamma = 0.98 # float\n",
    "    self._goal = 0 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)\n",
    "\n",
    "    # Build the maze\n",
    "    self._build_maze()\n",
    "                              \n",
    "\n",
    "  # Functions used to build the Maze environment \n",
    "  # You DO NOT NEED to modify them\n",
    "  def _build_maze(self):\n",
    "    \"\"\"\n",
    "    Maze initialisation.\n",
    "    input: /\n",
    "    output: /\n",
    "    \"\"\"\n",
    "\n",
    "    # Properties of the maze\n",
    "    self._shape = (13, 10)\n",
    "    self._obstacle_locs = [\n",
    "                          (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \\\n",
    "                          (2,1), (2,2), (2,3), (2,7), \\\n",
    "                          (3,1), (3,2), (3,3), (3,7), \\\n",
    "                          (4,1), (4,7), \\\n",
    "                          (5,1), (5,7), \\\n",
    "                          (6,5), (6,6), (6,7), \\\n",
    "                          (8,0), \\\n",
    "                          (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \\\n",
    "                          (10,0)\n",
    "                         ] # Location of obstacles\n",
    "    self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states\n",
    "    self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]\n",
    "    self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states\n",
    "    self._default_reward = -1 # Reward for each action performs in the environment\n",
    "    self._max_t = 500 # Max number of steps in the environment\n",
    "\n",
    "    # Actions\n",
    "    self._action_size = 4\n",
    "    self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on\n",
    "        \n",
    "    # States\n",
    "    self._locations = []\n",
    "    for i in range (self._shape[0]):\n",
    "      for j in range (self._shape[1]):\n",
    "        loc = (i,j) \n",
    "        # Adding the state to locations if it is no obstacle\n",
    "        if self._is_location(loc):\n",
    "          self._locations.append(loc)\n",
    "    self._state_size = len(self._locations)\n",
    "\n",
    "    # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)\n",
    "    self._neighbours = np.zeros((self._state_size, 4)) \n",
    "    \n",
    "    for state in range(self._state_size):\n",
    "      loc = self._get_loc_from_state(state)\n",
    "\n",
    "      # North\n",
    "      neighbour = (loc[0]-1, loc[1]) # North neighbours location\n",
    "      if self._is_location(neighbour):\n",
    "        self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)\n",
    "      else: # If there is no neighbour in this direction, coming back to current state\n",
    "        self._neighbours[state][self._direction_names.index('N')] = state\n",
    "\n",
    "      # East\n",
    "      neighbour = (loc[0], loc[1]+1) # East neighbours location\n",
    "      if self._is_location(neighbour):\n",
    "        self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)\n",
    "      else: # If there is no neighbour in this direction, coming back to current state\n",
    "        self._neighbours[state][self._direction_names.index('E')] = state\n",
    "\n",
    "      # South\n",
    "      neighbour = (loc[0]+1, loc[1]) # South neighbours location\n",
    "      if self._is_location(neighbour):\n",
    "        self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)\n",
    "      else: # If there is no neighbour in this direction, coming back to current state\n",
    "        self._neighbours[state][self._direction_names.index('S')] = state\n",
    "\n",
    "      # West\n",
    "      neighbour = (loc[0], loc[1]-1) # West neighbours location\n",
    "      if self._is_location(neighbour):\n",
    "        self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)\n",
    "      else: # If there is no neighbour in this direction, coming back to current state\n",
    "        self._neighbours[state][self._direction_names.index('W')] = state\n",
    "\n",
    "    # Absorbing\n",
    "    self._absorbing = np.zeros((1, self._state_size))\n",
    "    for a in self._absorbing_locs:\n",
    "      absorbing_state = self._get_state_from_loc(a)\n",
    "      self._absorbing[0, absorbing_state] = 1\n",
    "\n",
    "    # Transition matrix\n",
    "    self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A\n",
    "    for action in range(self._action_size):\n",
    "      for outcome in range(4): # For each direction (N, E, S, W)\n",
    "        # The agent has prob_success probability to go in the correct direction\n",
    "        if action == outcome:\n",
    "          prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)\n",
    "        # Equal probability to go into one of the other directions\n",
    "        else:\n",
    "          prob = (1.0 - self._prob_success) / 3.0\n",
    "          \n",
    "        # Write this probability in the transition matrix\n",
    "        for prior_state in range(self._state_size):\n",
    "          # If absorbing state, probability of 0 to go to any other states\n",
    "          if not self._absorbing[0, prior_state]:\n",
    "            post_state = self._neighbours[prior_state, outcome] # Post state number\n",
    "            post_state = int(post_state) # Transform in integer to avoid error\n",
    "            self._T[prior_state, post_state, action] += prob\n",
    "\n",
    "    # Reward matrix\n",
    "    self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1\n",
    "    self._R = self._default_reward * self._R # Set default_reward everywhere\n",
    "    for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards\n",
    "      post_state = self._get_state_from_loc(self._absorbing_locs[i])\n",
    "      self._R[:,post_state,:] = self._absorbing_rewards[i]\n",
    "\n",
    "    # Creating the graphical Maze world\n",
    "    self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)\n",
    "    \n",
    "    # Reset the environment\n",
    "    self.reset()\n",
    "\n",
    "\n",
    "  def _is_location(self, loc):\n",
    "    \"\"\"\n",
    "    Is the location a valid state (not out of Maze and not an obstacle)\n",
    "    input: loc {tuple} -- location of the state\n",
    "    output: _ {bool} -- is the location a valid state\n",
    "    \"\"\"\n",
    "    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):\n",
    "      return False\n",
    "    elif (loc in self._obstacle_locs):\n",
    "      return False\n",
    "    else:\n",
    "      return True\n",
    "\n",
    "\n",
    "  def _get_state_from_loc(self, loc):\n",
    "    \"\"\"\n",
    "    Get the state number corresponding to a given location\n",
    "    input: loc {tuple} -- location of the state\n",
    "    output: index {int} -- corresponding state number\n",
    "    \"\"\"\n",
    "    return self._locations.index(tuple(loc))\n",
    "\n",
    "\n",
    "  def _get_loc_from_state(self, state):\n",
    "    \"\"\"\n",
    "    Get the state number corresponding to a given location\n",
    "    input: index {int} -- state number\n",
    "    output: loc {tuple} -- corresponding location\n",
    "    \"\"\"\n",
    "    return self._locations[state]\n",
    "\n",
    "  # Getter functions used only for DP agents\n",
    "  # You DO NOT NEED to modify them\n",
    "  def get_T(self):\n",
    "    return self._T\n",
    "\n",
    "  def get_R(self):\n",
    "    return self._R\n",
    "\n",
    "  def get_absorbing(self):\n",
    "    return self._absorbing\n",
    "\n",
    "  # Getter functions used for DP, MC and TD agents\n",
    "  # You DO NOT NEED to modify them\n",
    "  def get_graphics(self):\n",
    "    return self._graphics\n",
    "\n",
    "  def get_action_size(self):\n",
    "    return self._action_size\n",
    "\n",
    "  def get_state_size(self):\n",
    "    return self._state_size\n",
    "\n",
    "  def get_gamma(self):\n",
    "    return self._gamma\n",
    "\n",
    "  # Functions used to perform episodes in the Maze environment\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment state to one of the possible starting states\n",
    "    input: /\n",
    "    output: \n",
    "      - t {int} -- current timestep\n",
    "      - state {int} -- current state of the envionment\n",
    "      - reward {int} -- current reward\n",
    "      - done {bool} -- True if reach a terminal state / 0 otherwise\n",
    "    \"\"\"\n",
    "    self._t = 0\n",
    "    self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])\n",
    "    self._reward = 0\n",
    "    self._done = False\n",
    "    return self._t, self._state, self._reward, self._done\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    Perform an action in the environment\n",
    "    input: action {int} -- action to perform\n",
    "    output: \n",
    "      - t {int} -- current timestep\n",
    "      - state {int} -- current state of the envionment\n",
    "      - reward {int} -- current reward\n",
    "      - done {bool} -- True if reach a terminal state / 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # If environment already finished, print an error\n",
    "    if self._done or self._absorbing[0, self._state]:\n",
    "      print(\"Please reset the environment\")\n",
    "      return self._t, self._state, self._reward, self._done\n",
    "\n",
    "    # Drawing a random number used for probaility of next state\n",
    "    probability_success = random.uniform(0,1)\n",
    "\n",
    "    # Look for the first possible next states (so get a reachable state even if probability_success = 0)\n",
    "    new_state = 0\n",
    "    while self._T[self._state, new_state, action] == 0: \n",
    "      new_state += 1\n",
    "    assert self._T[self._state, new_state, action] != 0, \"Selected initial state should be probability 0, something might be wrong in the environment.\"\n",
    "\n",
    "    # Find the first state for which probability of occurence matches the random value\n",
    "    total_probability = self._T[self._state, new_state, action]\n",
    "    while (total_probability < probability_success) and (new_state < self._state_size-1):\n",
    "     new_state += 1\n",
    "     total_probability += self._T[self._state, new_state, action]\n",
    "    assert self._T[self._state, new_state, action] != 0, \"Selected state should be probability 0, something might be wrong in the environment.\"\n",
    "    \n",
    "    # Setting new t, state, reward and done\n",
    "    self._t += 1\n",
    "    self._reward = self._R[self._state, new_state, action]\n",
    "    self._done = self._absorbing[0, new_state] or self._t > self._max_t\n",
    "    self._state = new_state\n",
    "    return self._t, self._state, self._reward, self._done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW3Ul0q-VRE-"
   },
   "source": [
    "## DP Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3ucYXx5NqStY"
   },
   "outputs": [],
   "source": [
    "# This class define the Dynamic Programing agent \n",
    "\n",
    "class DP_agent(object):\n",
    "\n",
    "  # [Action required]\n",
    "  # WARNING: make sure this function can be called by the auto-marking script\n",
    "  def solve(self, env):\n",
    "    \"\"\"\n",
    "    Solve a given Maze environment using Dynamic Programming\n",
    "    input: env {Maze object} -- Maze to solve\n",
    "    output: \n",
    "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
    "      - V {np.array} -- Corresponding value function \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialisation (can be edited)\n",
    "    policy = np.zeros((env.get_state_size(), env.get_action_size()))\n",
    "    V = np.zeros(env.get_state_size())\n",
    "\n",
    "    #### \n",
    "    # Add your code here\n",
    "    # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()\n",
    "    ####\n",
    "    \n",
    "    \n",
    "  def value_iteration(self, threshold = 0.0001, gamma = 0.98):\n",
    "    \"\"\"\n",
    "    Value iteration on GridWorld\n",
    "    input: \n",
    "      - threshold {float} -- threshold value used to stop the value iteration algorithm\n",
    "      - gamma {float} -- discount factor\n",
    "    output: \n",
    "      - policy {np.array} -- optimal policy found using the value iteration algorithm\n",
    "      - V {np.array} -- value function corresponding to the policy\n",
    "      - epochs {int} -- number of epochs to find this policy\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation\n",
    "    epochs = 0\n",
    "    threshold = 0.0001\n",
    "    delta = threshold # Setting value of delta to go through the first breaking condition\n",
    "    V = np.zeros(env.get_state_size()) # Initialise values at 0 for each state\n",
    "    gamma = env.get_gamma()\n",
    "    \n",
    "\n",
    "    while delta >= threshold:\n",
    "      delta = 0 # Reinitialise delta value\n",
    "\n",
    "      # For each state\n",
    "      for prior_state in range(env.get_state_size()):\n",
    "\n",
    "        # If not an absorbing state\n",
    "        if not env.get_absorbing()[0, prior_state]:\n",
    "                  \n",
    "          # Store the previous value for that state\n",
    "          v = V[prior_state] \n",
    "\n",
    "          # Compute Q value\n",
    "          Q = np.zeros(4) # Initialise with value 0\n",
    "          for post_state in range(env.get_state_size()):\n",
    "            Q += env.get_T()[prior_state, post_state,:] * (env.get_R()[prior_state, post_state, :] + gamma * V[post_state])\n",
    "                \n",
    "          # Set the new value to the maximum of Q\n",
    "          V[prior_state]= np.max(Q) \n",
    "\n",
    "          # Compute the new delta\n",
    "          delta = max(delta, np.abs(v - V[prior_state]))\n",
    "            \n",
    "            \n",
    "    # When the loop is finished, fill in the optimal policy\n",
    "    policy = np.zeros((env.get_state_size(), env.get_action_size())) # Initialisation\n",
    "\n",
    "    for prior_state in range(env.get_state_size()):\n",
    "      # Compute the Q value\n",
    "      Q = np.zeros(4)\n",
    "      for post_state in range(env.get_state_size()):\n",
    "        Q += env.get_T()[prior_state, post_state,:] * (env.get_R()[prior_state, post_state, :] + gamma * V[post_state])\n",
    "            \n",
    "      # The action that maximises the Q value gets probability 1\n",
    "      policy[prior_state, np.argmax(Q)] = 1 \n",
    "\n",
    "\n",
    "    return policy, V\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14i0zRkdVSqk"
   },
   "source": [
    "## MC agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CdHvHvcSrEW9"
   },
   "outputs": [],
   "source": [
    "# This class define the Monte-Carlo agent\n",
    "\n",
    "class MC_agent(object):\n",
    "  \n",
    "  # [Action required]\n",
    "  # WARNING: make sure this function can be called by the auto-marking script\n",
    "  def solve(self, env):\n",
    "    \"\"\"\n",
    "    Solve a given Maze environment using Monte Carlo learning\n",
    "    input: env {Maze object} -- Maze to solve\n",
    "    output: \n",
    "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
    "      - values {list of np.array} -- List of successive value functions for each episode \n",
    "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation (can be edited)\n",
    "    Q = np.random.rand(env.get_state_size(), env.get_action_size()) \n",
    "    V = np.zeros(env.get_state_size())\n",
    "    policy = np.zeros((env.get_state_size(), env.get_action_size())) \n",
    "    values = [V]\n",
    "    total_rewards = []\n",
    "\n",
    "    #### \n",
    "    # Add your code here\n",
    "    # WARNING: this agent only has access to env.reset() and env.step()\n",
    "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
    "    ####\n",
    "    \n",
    "    return policy, values, total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMkZKrh6VUgw"
   },
   "source": [
    "## TD agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Xyko9SvrGbE"
   },
   "outputs": [],
   "source": [
    "# This class define the Temporal-Difference agent\n",
    "\n",
    "class TD_agent(object):\n",
    "\n",
    "  # [Action required]\n",
    "  # WARNING: make sure this function can be called by the auto-marking script\n",
    "  def solve(self, env):\n",
    "    \"\"\"\n",
    "    Solve a given Maze environment using Temporal Difference learning\n",
    "    input: env {Maze object} -- Maze to solve\n",
    "    output: \n",
    "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
    "      - values {list of np.array} -- List of successive value functions for each episode \n",
    "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation (can be edited)\n",
    "    Q = np.random.rand(env.get_state_size(), env.get_action_size()) \n",
    "    V = np.zeros(env.get_state_size())\n",
    "    policy = np.zeros((env.get_state_size(), env.get_action_size())) \n",
    "    values = [V]\n",
    "    total_rewards = []\n",
    "\n",
    "    #### \n",
    "    # Add your code here\n",
    "    # WARNING: this agent only has access to env.reset() and env.step()\n",
    "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
    "    ####\n",
    "    \n",
    "    return policy, values, total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzSzRSO6VWVD"
   },
   "source": [
    "## Example main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eyeJfvwXp3ta",
    "outputId": "229af227-7973-4819-dc05-ff8219987805",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Maze:\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Maze' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example main (can be edited)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m### Question 0: Defining the environment\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating the Maze:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m maze \u001b[38;5;241m=\u001b[39m \u001b[43mMaze\u001b[49m()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m### Question 1: Dynamic programming\u001b[39;00m\n\u001b[1;32m     11\u001b[0m dp_agent \u001b[38;5;241m=\u001b[39m DP_agent()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Maze' is not defined"
     ]
    }
   ],
   "source": [
    "# Example main (can be edited)\n",
    "\n",
    "### Question 0: Defining the environment\n",
    "\n",
    "print(\"Creating the Maze:\\n\")\n",
    "maze = Maze()\n",
    "\n",
    "\n",
    "### Question 1: Dynamic programming\n",
    "\n",
    "dp_agent = DP_agent()\n",
    "dp_policy, dp_value = dp_agent.solve(maze)\n",
    "\n",
    "print(\"Results of the DP agent:\\n\")\n",
    "maze.get_graphics().draw_policy(dp_policy)\n",
    "maze.get_graphics().draw_value(dp_value)\n",
    "\n",
    "\n",
    "### Question 2: Monte-Carlo learning\n",
    "\n",
    "mc_agent = MC_agent()\n",
    "mc_policy, mc_values, total_rewards = mc_agent.solve(maze)\n",
    "\n",
    "print(\"Results of the MC agent:\\n\")\n",
    "maze.get_graphics().draw_policy(mc_policy)\n",
    "maze.get_graphics().draw_value(mc_values[-1])\n",
    "\n",
    "\n",
    "### Question 3: Temporal-Difference learning\n",
    "\n",
    "td_agent = TD_agent()\n",
    "td_policy, td_values, total_rewards = td_agent.solve(maze)\n",
    "\n",
    "print(\"Results of the TD agent:\\n\")\n",
    "maze.get_graphics().draw_policy(td_policy)\n",
    "maze.get_graphics().draw_value(td_values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lbY8DCqoVJlw",
    "DW3Ul0q-VRE-",
    "14i0zRkdVSqk"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
